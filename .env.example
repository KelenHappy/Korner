# SnapAsk Environment Configuration
# Copy this file to .env and fill in your actual values

# AMD LLM Endpoint (OpenAI-compatible)
# Example: https://api.amd-inference.com/v1/chat/completions
AMD_LLM_ENDPOINT=https://your-amd-endpoint.com/v1/chat/completions

# AMD API Key for authentication
AMD_API_KEY=your-api-key-here

# Model name (defaults to "gpt-oss-120b" if not set)
MODEL_NAME=gpt-oss-120b

# Optional: Timeout for LLM requests (seconds)
# LLM_TIMEOUT=60

# Optional: Max tokens in response
# MAX_TOKENS=2048

# Optional: Temperature (0.0-2.0, lower = more deterministic)
# TEMPERATURE=0.2
